{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torchvision\n", "from pcs.models import (CosineClassifier, MemoryBank, SSDALossModule,\n", "                        compute_variance, loss_info, torch_kmeans,\n", "                        update_data_memory)\n", "from pcs.utils import (AverageMeter, datautils, is_div, per, reverse_domain,\n", "                       torchutils, utils)\n", "from sklearn import metrics\n", "from tqdm import tqdm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from . import BaseAgent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ls_abbr = {\n", "    \"cls-so\": \"cls\",\n", "    \"proto-each\": \"P\",\n", "    \"proto-src\": \"Ps\",\n", "    \"proto-tgt\": \"Pt\",\n", "    \"cls-info\": \"info\",\n", "    \"I2C-cross\": \"C\",\n", "    \"semi-condentmax\": \"sCE\",\n", "    \"semi-entmin\": \"sE\",\n", "    \"tgt-condentmax\": \"tCE\",\n", "    \"tgt-entmin\": \"tE\",\n", "    \"ID-each\": \"I\",\n", "    \"CD-cross\": \"CD\",\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class CDSAgent(BaseAgent):\n", "    def __init__(self, config):\n", "        self.config = config\n", "        self._define_task(config)\n", "        self.is_features_computed = False\n", "        self.current_iteration_source = self.current_iteration_target = 0\n", "        self.domain_map = {\n", "            \"source\": self.config.data_params.source,\n", "            \"target\": self.config.data_params.target,\n", "        }\n", "        super(CDSAgent, self).__init__(config)\n\n", "        # for MIM\n", "        self.momentum_softmax_target = torchutils.MomentumSoftmax(\n", "            self.num_class, m=len(self.get_attr(\"target\", \"train_loader\"))\n", "        )\n", "        self.momentum_softmax_source = torchutils.MomentumSoftmax(\n", "            self.num_class, m=len(self.get_attr(\"source\", \"train_loader\"))\n", "        )\n\n", "        # init loss\n", "        loss_fn = SSDALossModule(self.config, gpu_devices=self.gpu_devices)\n", "        loss_fn = nn.DataParallel(loss_fn, device_ids=self.gpu_devices).cuda()\n", "        self.loss_fn = loss_fn\n", "        if self.config.pretrained_exp_dir is None:\n", "            self._init_memory_bank()\n\n", "        # init statics\n", "        self._init_labels()\n", "        self._load_fewshot_to_cls_weight()\n", "    def _define_task(self, config):\n", "        # specify task\n", "        self.fewshot = config.data_params.fewshot\n", "        self.clus = config.loss_params.clus != None\n", "        self.cls = self.semi = self.tgt = self.ssl = False\n", "        self.is_pseudo_src = self.is_pseudo_tgt = False\n", "        for ls in config.loss_params.loss:\n", "            self.cls = self.cls | ls.startswith(\"cls\")\n", "            self.semi = self.semi | ls.startswith(\"semi\")\n", "            self.tgt = self.tgt | ls.startswith(\"tgt\")\n", "            self.ssl = self.ssl | (ls.split(\"-\")[0] not in [\"cls\", \"semi\", \"tgt\"])\n", "            self.is_pseudo_src = self.is_pseudo_src | ls.startswith(\"semi-pseudo\")\n", "            self.is_pseudo_tgt = self.is_pseudo_tgt | ls.startswith(\"tgt-pseudo\")\n", "        self.is_pseudo_src = self.is_pseudo_src | (\n", "            config.loss_params.pseudo and self.fewshot is not None\n", "        )\n", "        self.is_pseudo_tgt = self.is_pseudo_tgt | config.loss_params.pseudo\n", "        self.semi = self.semi | self.is_pseudo_src\n", "        if self.clus:\n", "            self.is_pseudo_tgt = self.is_pseudo_tgt | (\n", "                config.loss_params.clus.tgt_GC == \"PGC\" and \"GC\" in config.clus.type\n", "            )\n", "    def _init_labels(self):\n", "        train_len_tgt = self.get_attr(\"target\", \"train_len\")\n", "        train_len_src = self.get_attr(\"source\", \"train_len\")\n\n", "        # labels for pseudo\n", "        if self.fewshot:\n", "            self.predict_ordered_labels_pseudo_source = (\n", "                torch.zeros(train_len_src, dtype=torch.long).detach().cuda() - 1\n", "            )\n", "            for ind, lbl in zip(self.fewshot_index_source, self.fewshot_label_source):\n", "                self.predict_ordered_labels_pseudo_source[ind] = lbl\n", "        self.predict_ordered_labels_pseudo_target = (\n", "            torch.zeros(train_len_tgt, dtype=torch.long).detach().cuda() - 1\n", "        )\n", "    def _load_datasets(self):\n", "        name = self.config.data_params.name\n", "        num_workers = self.config.data_params.num_workers\n", "        fewshot = self.config.data_params.fewshot\n", "        domain = self.domain_map\n", "        image_size = self.config.data_params.image_size\n", "        aug_src = self.config.data_params.aug_src\n", "        aug_tgt = self.config.data_params.aug_tgt\n", "        raw = \"raw\"\n", "        self.num_class = datautils.get_class_num(\n", "            f'data/splits/{name}/{domain[\"source\"]}.txt'\n", "        )\n", "        self.class_map = datautils.get_class_map(\n", "            f'data/splits/{name}/{domain[\"target\"]}.txt'\n", "        )\n", "        batch_size_dict = {\n", "            \"test\": self.config.optim_params.batch_size,\n", "            \"source\": self.config.optim_params.batch_size_src,\n", "            \"target\": self.config.optim_params.batch_size_tgt,\n", "            \"labeled\": self.config.optim_params.batch_size_lbd,\n", "        }\n", "        self.batch_size_dict = batch_size_dict\n\n", "        # self-supervised Dataset\n", "        for domain_name in (\"source\", \"target\"):\n", "            aug_name = {\"source\": aug_src, \"target\": aug_tgt}[domain_name]\n\n", "            # Training datasets\n", "            train_dataset = datautils.create_dataset(\n", "                name,\n", "                domain[domain_name],\n", "                suffix=\"\",\n", "                ret_index=True,\n", "                image_transform=aug_name,\n", "                use_mean_std=False,\n", "                image_size=image_size,\n", "            )\n", "            train_loader = datautils.create_loader(\n", "                train_dataset,\n", "                batch_size_dict[domain_name],\n", "                is_train=True,\n", "                num_workers=num_workers,\n", "            )\n", "            train_init_loader = datautils.create_loader(\n", "                train_dataset,\n", "                batch_size_dict[domain_name],\n", "                is_train=False,\n", "                num_workers=num_workers,\n", "            )\n", "            train_labels = torch.from_numpy(train_dataset.labels).detach().cuda()\n", "            self.set_attr(domain_name, \"train_dataset\", train_dataset)\n", "            self.set_attr(domain_name, \"train_ordered_labels\", train_labels)\n", "            self.set_attr(domain_name, \"train_loader\", train_loader)\n", "            self.set_attr(domain_name, \"train_init_loader\", train_init_loader)\n", "            self.set_attr(domain_name, \"train_len\", len(train_dataset))\n\n", "        # Classification and Fewshot Dataset\n", "        if fewshot:\n", "            train_lbd_dataset_source = datautils.create_dataset(\n", "                name,\n", "                domain[\"source\"],\n", "                suffix=f\"labeled_{fewshot}\",\n", "                ret_index=True,\n", "                image_transform=aug_src,\n", "                image_size=image_size,\n", "            )\n", "            src_dataset = self.get_attr(\"source\", \"train_dataset\")\n", "            (\n", "                self.fewshot_index_source,\n", "                self.fewshot_label_source,\n", "            ) = datautils.get_fewshot_index(train_lbd_dataset_source, src_dataset)\n", "            test_unl_dataset_source = datautils.create_dataset(\n", "                name,\n", "                domain[\"source\"],\n", "                suffix=f\"unlabeled_{fewshot}\",\n", "                ret_index=True,\n", "                image_transform=raw,\n", "                image_size=image_size,\n", "            )\n", "            self.test_unl_loader_source = datautils.create_loader(\n", "                test_unl_dataset_source,\n", "                batch_size_dict[\"test\"],\n", "                is_train=False,\n", "                num_workers=num_workers,\n", "            )\n\n", "            # labels for fewshot\n", "            train_len = self.get_attr(\"source\", \"train_len\")\n", "            self.fewshot_labels = (\n", "                torch.zeros(train_len, dtype=torch.long).detach().cuda() - 1\n", "            )\n", "            for ind, lbl in zip(self.fewshot_index_source, self.fewshot_label_source):\n", "                self.fewshot_labels[ind] = lbl\n", "        else:\n", "            train_lbd_dataset_source = datautils.create_dataset(\n", "                name,\n", "                domain[\"source\"],\n", "                ret_index=True,\n", "                image_transform=aug_src,\n", "                image_size=image_size,\n", "            )\n", "        test_suffix = \"test\" if self.config.data_params.train_val_split else \"\"\n", "        test_unl_dataset_target = datautils.create_dataset(\n", "            name,\n", "            domain[\"target\"],\n", "            suffix=test_suffix,\n", "            ret_index=True,\n", "            image_transform=raw,\n", "            image_size=image_size,\n", "        )\n", "        self.train_lbd_loader_source = datautils.create_loader(\n", "            train_lbd_dataset_source,\n", "            batch_size_dict[\"labeled\"],\n", "            num_workers=num_workers,\n", "        )\n", "        self.test_unl_loader_target = datautils.create_loader(\n", "            test_unl_dataset_target,\n", "            batch_size_dict[\"test\"],\n", "            is_train=False,\n", "            num_workers=num_workers,\n", "        )\n", "        self.logger.info(\n", "            f\"Dataset {name}, source {self.config.data_params.source}, target {self.config.data_params.target}\"\n", "        )\n", "    def _create_model(self):\n", "        version_grp = self.config.model_params.version.split(\"-\")\n", "        version = version_grp[-1]\n", "        pretrained = \"pretrain\" in version_grp\n", "        if pretrained:\n", "            self.logger.info(\"Imagenet pretrained model used\")\n", "        out_dim = self.config.model_params.out_dim\n\n", "        # backbone\n", "        if \"resnet\" in version:\n", "            net_class = getattr(torchvision.models, version)\n", "            if pretrained:\n", "                model = net_class(pretrained=pretrained)\n", "                model.fc = nn.Linear(model.fc.in_features, out_dim)\n", "                torchutils.weights_init(model.fc)\n", "            else:\n", "                model = net_class(pretrained=False, num_classes=out_dim)\n", "        else:\n", "            raise NotImplementedError\n", "        model = nn.DataParallel(model, device_ids=self.gpu_devices)\n", "        model = model.cuda()\n", "        self.model = model\n\n", "        # classification head\n", "        if self.cls:\n", "            self.criterion = nn.CrossEntropyLoss().cuda()\n", "            cls_head = CosineClassifier(\n", "                num_class=self.num_class, inc=out_dim, temp=self.config.loss_params.T\n", "            )\n", "            torchutils.weights_init(cls_head)\n", "            self.cls_head = cls_head.cuda()\n", "    def _create_optimizer(self):\n", "        lr = self.config.optim_params.learning_rate\n", "        momentum = self.config.optim_params.momentum\n", "        weight_decay = self.config.optim_params.weight_decay\n", "        conv_lr_ratio = self.config.optim_params.conv_lr_ratio\n", "        parameters = []\n", "        # batch_norm layer: no weight_decay\n", "        params_bn, _ = torchutils.split_params_by_name(self.model, \"bn\")\n", "        parameters.append({\"params\": params_bn, \"weight_decay\": 0.0})\n", "        # conv layer: small lr\n", "        _, params_conv = torchutils.split_params_by_name(self.model, [\"fc\", \"bn\"])\n", "        if conv_lr_ratio:\n", "            parameters[0][\"lr\"] = lr * conv_lr_ratio\n", "            parameters.append({\"params\": params_conv, \"lr\": lr * conv_lr_ratio})\n", "        else:\n", "            parameters.append({\"params\": params_conv})\n", "        # fc layer\n", "        params_fc, _ = torchutils.split_params_by_name(self.model, \"fc\")\n", "        if self.cls and self.config.optim_params.cls_update:\n", "            params_fc.extend(list(self.cls_head.parameters()))\n", "        parameters.append({\"params\": params_fc})\n", "        self.optim = torch.optim.SGD(\n", "            parameters,\n", "            lr=lr,\n", "            weight_decay=weight_decay,\n", "            momentum=momentum,\n", "            nesterov=self.config.optim_params.nesterov,\n", "        )\n\n", "        # lr schedular\n", "        if self.config.optim_params.lr_decay_schedule:\n", "            optim_stepLR = torch.optim.lr_scheduler.MultiStepLR(\n", "                self.optim,\n", "                milestones=self.config.optim_params.lr_decay_schedule,\n", "                gamma=self.config.optim_params.lr_decay_rate,\n", "            )\n", "            self.lr_scheduler_list.append(optim_stepLR)\n", "        if self.config.optim_params.decay:\n", "            self.optim_iterdecayLR = torchutils.lr_scheduler_invLR(self.optim)\n", "    def train_one_epoch(self):\n", "        # train preparation\n", "        self.model = self.model.train()\n", "        if self.cls:\n", "            self.cls_head.train()\n", "        self.loss_fn.module.epoch = self.current_epoch\n", "        loss_list = self.config.loss_params.loss\n", "        loss_weight = self.config.loss_params.weight\n", "        loss_warmup = self.config.loss_params.start\n", "        loss_giveup = self.config.loss_params.end\n", "        num_loss = len(loss_list)\n", "        source_loader = self.get_attr(\"source\", \"train_loader\")\n", "        target_loader = self.get_attr(\"target\", \"train_loader\")\n", "        if self.config.steps_epoch is None:\n", "            num_batches = max(len(source_loader), len(target_loader)) + 1\n", "            self.logger.info(f\"source loader batches: {len(source_loader)}\")\n", "            self.logger.info(f\"target loader batches: {len(target_loader)}\")\n", "        else:\n", "            num_batches = self.config.steps_epoch\n", "        epoch_loss = AverageMeter()\n", "        epoch_loss_parts = [AverageMeter() for _ in range(num_loss)]\n\n", "        # cluster\n", "        if self.clus:\n", "            if self.config.loss_params.clus.kmeans_freq:\n", "                kmeans_batches = num_batches // self.config.loss_params.clus.kmeans_freq\n", "            else:\n", "                kmeans_batches = 1\n", "        else:\n", "            kmeans_batches = None\n\n", "        # load weight\n", "        self._load_fewshot_to_cls_weight()\n", "        if self.fewshot:\n", "            fewshot_index = torch.tensor(self.fewshot_index_source).cuda()\n", "        tqdm_batch = tqdm(\n", "            total=num_batches, desc=f\"[Epoch {self.current_epoch}]\", leave=False\n", "        )\n", "        tqdm_post = {}\n", "        for batch_i in range(num_batches):\n", "            # Kmeans\n", "            if is_div(kmeans_batches, batch_i):\n", "                self._update_cluster_labels()\n", "            if not self.config.optim_params.cls_update:\n", "                self._load_fewshot_to_cls_weight()\n\n", "            # iteration over all source images\n", "            if not batch_i % len(source_loader):\n", "                source_iter = iter(source_loader)\n", "                if \"semi-condentmax\" in self.config.loss_params.loss:\n", "                    momentum_prob_source = (\n", "                        self.momentum_softmax_source.softmax_vector.cuda()\n", "                    )\n", "                    self.momentum_softmax_source.reset()\n\n", "            # iteration over all target images\n", "            if not batch_i % len(target_loader):\n", "                target_iter = iter(target_loader)\n", "                if \"tgt-condentmax\" in self.config.loss_params.loss:\n", "                    momentum_prob_target = (\n", "                        self.momentum_softmax_target.softmax_vector.cuda()\n", "                    )\n", "                    self.momentum_softmax_target.reset()\n\n", "            # iteration over all labeled source images\n", "            if self.cls and not batch_i % len(self.train_lbd_loader_source):\n", "                source_lbd_iter = iter(self.train_lbd_loader_source)\n\n", "            # calculate loss\n", "            for domain_name in (\"source\", \"target\"):\n", "                loss = torch.tensor(0).cuda()\n", "                loss_d = 0\n", "                loss_part_d = [0] * num_loss\n", "                batch_size = self.batch_size_dict[domain_name]\n", "                if self.cls and domain_name == \"source\":\n", "                    indices_lbd, images_lbd, labels_lbd = next(source_lbd_iter)\n", "                    indices_lbl = indices_lbd.cuda()\n", "                    images_lbd = images_lbd.cuda()\n", "                    labels_lbd = labels_lbd.cuda()\n", "                    feat_lbd = self.model(images_lbd)\n", "                    feat_lbd = F.normalize(feat_lbd, dim=1)\n", "                    out_lbd = self.cls_head(feat_lbd)\n", "                # Matching & ssl\n", "                if (self.tgt and domain_name == \"target\") or self.ssl:\n", "                    loader_iter = (\n", "                        source_iter if domain_name == \"source\" else target_iter\n", "                    )\n", "                    indices_unl, images_unl, _ = next(loader_iter)\n", "                    images_unl = images_unl.cuda()\n", "                    indices_unl = indices_unl.cuda()\n", "                    feat_unl = self.model(images_unl)\n", "                    feat_unl = F.normalize(feat_unl, dim=1)\n", "                    out_unl = self.cls_head(feat_unl)\n", "                # Semi Supervised\n", "                if self.semi and domain_name == \"source\":\n", "                    semi_mask = ~torchutils.isin(indices_unl, fewshot_index)\n", "                    indices_semi = indices_unl[semi_mask]\n", "                    out_semi = out_unl[semi_mask]\n", "                # Self-supervised Learning\n", "                if self.ssl:\n", "                    _, new_data_memory, loss_ssl, aux_list = self.loss_fn(\n", "                        indices_unl, feat_unl, domain_name, self.parallel_helper_idxs\n", "                    )\n", "                    loss_ssl = [torch.mean(ls) for ls in loss_ssl]\n", "                # pseudo\n", "                loss_pseudo = torch.tensor(0).cuda()\n", "                is_pseudo = {\"source\": self.is_pseudo_src, \"target\": self.is_pseudo_tgt}\n", "                thres_dict = {\n", "                    \"source\": self.config.loss_params.thres_src,\n", "                    \"target\": self.config.loss_params.thres_tgt,\n", "                }\n", "                if is_pseudo[domain_name]:\n", "                    if domain_name == \"source\":\n", "                        indices_pseudo = indices_semi\n", "                        out_pseudo = out_semi\n", "                        pseudo_domain = self.predict_ordered_labels_pseudo_source\n", "                    else:\n", "                        indices_pseudo = indices_unl\n", "                        out_pseudo = out_unl  # [bs, class_num]\n", "                        pseudo_domain = self.predict_ordered_labels_pseudo_target\n", "                    thres = thres_dict[domain_name]\n", "                    # calculate loss\n", "                    loss_pseudo, aux = torchutils.pseudo_label_loss(\n", "                        out_pseudo,\n", "                        thres=thres,\n", "                        mask=None,\n", "                        num_class=self.num_class,\n", "                        aux=True,\n", "                    )\n", "                    mask_pseudo = aux[\"mask\"]\n", "                    # fewshot memory bank\n", "                    mb = self.get_attr(\"source\", \"memory_bank_wrapper\")\n", "                    indices_lbd_tounl = fewshot_index[indices_lbd]\n", "                    mb_feat_lbd = mb.at_idxs(indices_lbd_tounl)\n", "                    fewshot_data_memory = update_data_memory(mb_feat_lbd, feat_lbd)\n", "                    # stat\n", "                    pred_selected = out_pseudo.argmax(dim=1)[mask_pseudo]\n", "                    indices_selected = indices_pseudo[mask_pseudo]\n", "                    indices_unselected = indices_pseudo[~mask_pseudo]\n", "                    pseudo_domain[indices_selected] = pred_selected\n", "                    pseudo_domain[indices_unselected] = -1\n", "                # Compute Loss\n", "                for ind, ls in enumerate(loss_list):\n", "                    if (\n", "                        self.current_epoch < loss_warmup[ind]\n", "                        or self.current_epoch >= loss_giveup[ind]\n", "                    ):\n", "                        continue\n", "                    loss_part = torch.tensor(0).cuda()\n", "                    # *** handler for different loss ***\n", "                    # classification on few-shot\n", "                    if ls == \"cls-so\" and domain_name == \"source\":\n", "                        loss_part = self.criterion(out_lbd, labels_lbd)\n", "                    elif ls == \"cls-info\" and domain_name == \"source\":\n", "                        loss_part = loss_info(feat_lbd, mb_feat_lbd, labels_lbd)\n", "                    # semi-supervision learning on unlabled source\n", "                    elif ls == \"semi-entmin\" and domain_name == \"source\":\n", "                        loss_part = torchutils.entropy(out_semi)\n", "                    elif ls == \"semi-condentmax\" and domain_name == \"source\":\n", "                        bs = out_semi.size(0)\n", "                        prob_semi = F.softmax(out_semi, dim=1)\n", "                        prob_mean_semi = prob_semi.sum(dim=0) / bs\n", "                        # update momentum\n", "                        self.momentum_softmax_source.update(\n", "                            prob_mean_semi.cpu().detach(), bs\n", "                        )\n", "                        # get momentum probability\n", "                        momentum_prob_source = (\n", "                            self.momentum_softmax_source.softmax_vector.cuda()\n", "                        )\n", "                        # compute loss\n", "                        entropy_cond = -torch.sum(\n", "                            prob_mean_semi * torch.log(momentum_prob_source + 1e-5)\n", "                        )\n", "                        loss_part = -entropy_cond\n", "                    # learning on unlabeled target domain\n", "                    elif ls == \"tgt-entmin\" and domain_name == \"target\":\n", "                        loss_part = torchutils.entropy(out_unl)\n", "                    elif ls == \"tgt-condentmax\" and domain_name == \"target\":\n", "                        bs = out_unl.size(0)\n", "                        prob_unl = F.softmax(out_unl, dim=1)\n", "                        prob_mean_unl = prob_unl.sum(dim=0) / bs\n", "                        # update momentum\n", "                        self.momentum_softmax_target.update(\n", "                            prob_mean_unl.cpu().detach(), bs\n", "                        )\n", "                        # get momentum probability\n", "                        momentum_prob_target = (\n", "                            self.momentum_softmax_target.softmax_vector.cuda()\n", "                        )\n", "                        # compute loss\n", "                        entropy_cond = -torch.sum(\n", "                            prob_mean_unl * torch.log(momentum_prob_target + 1e-5)\n", "                        )\n", "                        loss_part = -entropy_cond\n", "                    # self-supervised learning\n", "                    elif ls.split(\"-\")[0] in [\"ID\", \"CD\", \"proto\", \"I2C\", \"C2C\"]:\n", "                        loss_part = loss_ssl[ind]\n", "                    loss_part = loss_weight[ind] * loss_part\n", "                    loss = loss + loss_part\n", "                    loss_d = loss_d + loss_part.item()\n", "                    loss_part_d[ind] = loss_part.item()\n", "                # Backpropagation\n", "                self.optim.zero_grad()\n", "                if len(loss_list) and loss != 0:\n", "                    loss.backward()\n", "                self.optim.step()\n", "                # update memory_bank\n", "                if self.ssl:\n", "                    self._update_memory_bank(domain_name, indices_unl, new_data_memory)\n", "                    if domain_name == \"source\":\n", "                        self._update_memory_bank(\n", "                            domain_name, indices_lbd_tounl, fewshot_data_memory\n", "                        )\n", "                # update lr info\n", "                tqdm_post[\"lr\"] = torchutils.get_lr(self.optim, g_id=-1)\n", "                # update loss info\n", "                epoch_loss.update(loss_d, batch_size)\n", "                tqdm_post[\"loss\"] = epoch_loss.avg\n", "                self.summary_writer.add_scalars(\n", "                    \"train/loss\", {\"loss\": epoch_loss.val}, self.current_iteration\n", "                )\n", "                self.train_loss.append(epoch_loss.val)\n", "                # update loss part info\n", "                domain_iteration = self.get_attr(domain_name, \"current_iteration\")\n", "                self.summary_writer.add_scalars(\n", "                    f\"train/{self.domain_map[domain_name]}_loss\",\n", "                    {\"loss\": epoch_loss.val},\n", "                    domain_iteration,\n", "                )\n", "                for i, ls in enumerate(loss_part_d):\n", "                    ls_name = loss_list[i]\n", "                    epoch_loss_parts[i].update(ls, batch_size)\n", "                    tqdm_post[ls_abbr[ls_name]] = epoch_loss_parts[i].avg\n", "                    self.summary_writer.add_scalars(\n", "                        f\"train/{self.domain_map[domain_name]}_loss\",\n", "                        {ls_name: epoch_loss_parts[i].val},\n", "                        domain_iteration,\n", "                    )\n", "                # adjust lr\n", "                if self.config.optim_params.decay:\n", "                    self.optim_iterdecayLR.step()\n", "                self.current_iteration += 1\n", "            tqdm_batch.set_postfix(tqdm_post)\n", "            tqdm_batch.update()\n", "            self.current_iteration_source += 1\n", "            self.current_iteration_target += 1\n", "        tqdm_batch.close()\n", "        self.current_loss = epoch_loss.avg\n", "    @torch.no_grad()\n", "    def _load_fewshot_to_cls_weight(self):\n", "        \"\"\"load centroids to cosine classifier\n", "        Args:\n", "            method (str, optional): None, 'fewshot', 'src', 'tgt'. Defaults to None.\n", "        \"\"\"\n", "        method = self.config.model_params.load_weight\n", "        if method is None:\n", "            return\n", "        assert method in [\"fewshot\", \"src\", \"tgt\", \"src-tgt\", \"fewshot-tgt\"]\n", "        thres = {\"src\": 1, \"tgt\": self.config.model_params.load_weight_thres}\n", "        bank = {\n", "            \"src\": self.get_attr(\"source\", \"memory_bank_wrapper\").as_tensor(),\n", "            \"tgt\": self.get_attr(\"target\", \"memory_bank_wrapper\").as_tensor(),\n", "        }\n", "        fewshot_label = {}\n", "        fewshot_index = {}\n", "        is_tgt = (\n", "            method in [\"tgt\", \"fewshot-tgt\", \"src-tgt\"]\n", "            and self.current_epoch >= self.config.model_params.load_weight_epoch\n", "        )\n", "        if method in [\"fewshot\", \"fewshot-tgt\"]:\n", "            if self.fewshot:\n", "                fewshot_label[\"src\"] = torch.tensor(self.fewshot_label_source)\n", "                fewshot_index[\"src\"] = torch.tensor(self.fewshot_index_source)\n", "            else:\n", "                fewshot_label[\"src\"] = self.get_attr(\"source\", \"train_ordered_labels\")\n", "                fewshot_index[\"src\"] = torch.arange(\n", "                    self.get_attr(\"source\", \"train_len\")\n", "                )\n", "        else:\n", "            mask = self.predict_ordered_labels_pseudo_source != -1\n", "            fewshot_label[\"src\"] = self.predict_ordered_labels_pseudo_source[mask]\n", "            fewshot_index[\"src\"] = mask.nonzero().squeeze(1)\n", "        if is_tgt:\n", "            mask = self.predict_ordered_labels_pseudo_target != -1\n", "            fewshot_label[\"tgt\"] = self.predict_ordered_labels_pseudo_target[mask]\n", "            fewshot_index[\"tgt\"] = mask.nonzero().squeeze(1)\n", "        for domain in (\"src\", \"tgt\"):\n", "            if domain == \"tgt\" and not is_tgt:\n", "                break\n", "            if domain == \"src\" and method == \"tgt\":\n", "                break\n", "            weight = self.cls_head.fc.weight.data\n", "            for label in range(self.num_class):\n", "                fewshot_mask = fewshot_label[domain] == label\n", "                if fewshot_mask.sum() < thres[domain]:\n", "                    continue\n", "                fewshot_ind = fewshot_index[domain][fewshot_mask]\n", "                bank_vec = bank[domain][fewshot_ind]\n", "                weight[label] = F.normalize(torch.mean(bank_vec, dim=0), dim=0)\n\n", "    # Validate\n", "    @torch.no_grad()\n", "    def validate(self):\n", "        self.model.eval()\n\n", "        # Domain Adaptation\n", "        if self.cls:\n", "            # self._load_fewshot_to_cls_weight()\n", "            self.cls_head.eval()\n", "            if (\n", "                self.config.data_params.fewshot\n", "                and self.config.data_params.name not in [\"visda17\", \"digits\"]\n", "            ):\n", "                self.score(\n", "                    self.test_unl_loader_source,\n", "                    name=f\"unlabeled {self.domain_map['source']}\",\n", "                )\n", "            self.current_val_metric = self.score(\n", "                self.test_unl_loader_target,\n", "                name=f\"unlabeled {self.domain_map['target']}\",\n", "            )\n\n", "        # update information\n", "        self.current_val_iteration += 1\n", "        if self.current_val_metric >= self.best_val_metric:\n", "            self.best_val_metric = self.current_val_metric\n", "            self.best_val_epoch = self.current_epoch\n", "            self.iter_with_no_improv = 0\n", "        else:\n", "            self.iter_with_no_improv += 1\n", "        self.val_acc.append(self.current_val_metric)\n", "        self.clear_train_features()\n", "    @torch.no_grad()\n", "    def score(self, loader, name=\"test\"):\n", "        correct = 0\n", "        size = 0\n", "        epoch_loss = AverageMeter()\n", "        error_indices = []\n", "        confusion_matrix = torch.zeros(self.num_class, self.num_class, dtype=torch.long)\n", "        pred_score = []\n", "        pred_label = []\n", "        label = []\n", "        for batch_i, (indices, images, labels) in enumerate(loader):\n", "            images = images.cuda()\n", "            labels = labels.cuda()\n", "            feat = self.model(images)\n", "            feat = F.normalize(feat, dim=1)\n", "            output = self.cls_head(feat)\n", "            prob = F.softmax(output, dim=-1)\n", "            loss = self.criterion(output, labels)\n", "            pred = torch.max(output, dim=1)[1]\n", "            pred_label.extend(pred.cpu().tolist())\n", "            label.extend(labels.cpu().tolist())\n", "            if self.num_class == 2:\n", "                pred_score.extend(prob[:, 1].cpu().tolist())\n", "            correct += pred.eq(labels).sum().item()\n", "            for t, p, ind in zip(labels, pred, indices):\n", "                confusion_matrix[t.long(), p.long()] += 1\n", "                if t != p:\n", "                    error_indices.append((ind, p))\n", "            size += pred.size(0)\n", "            epoch_loss.update(loss, pred.size(0))\n", "        acc = correct / size\n", "        self.summary_writer.add_scalars(\n", "            \"test/acc\", {f\"{name}\": acc}, self.current_epoch\n", "        )\n", "        self.summary_writer.add_scalars(\n", "            \"test/loss\", {f\"{name}\": epoch_loss.avg}, self.current_epoch\n", "        )\n", "        self.logger.info(\n", "            f\"[Epoch {self.current_epoch} {name}] loss={epoch_loss.avg:.5f}, acc={correct}/{size}({100. * acc:.3f}%)\"\n", "        )\n", "        return acc\n\n", "    # Load & Save checkpoint\n", "    def load_checkpoint(\n", "        self,\n", "        filename,\n", "        checkpoint_dir=None,\n", "        load_memory_bank=False,\n", "        load_model=True,\n", "        load_optim=False,\n", "        load_epoch=False,\n", "        load_cls=True,\n", "    ):\n", "        checkpoint_dir = checkpoint_dir or self.config.checkpoint_dir\n", "        filename = os.path.join(checkpoint_dir, filename)\n", "        try:\n", "            self.logger.info(f\"Loading checkpoint '{filename}'\")\n", "            checkpoint = torch.load(filename, map_location=\"cpu\")\n", "            if load_epoch:\n", "                self.current_epoch = checkpoint[\"epoch\"]\n", "                for domain_name in (\"source\", \"target\"):\n", "                    self.set_attr(\n", "                        domain_name,\n", "                        \"current_iteration\",\n", "                        checkpoint[f\"iteration_{domain_name}\"],\n", "                    )\n", "                self.current_iteration = checkpoint[\"iteration\"]\n", "                self.current_val_iteration = checkpoint[\"val_iteration\"]\n", "            if load_model:\n", "                model_state_dict = checkpoint[\"model_state_dict\"]\n", "                self.model.load_state_dict(model_state_dict)\n", "            if load_cls and self.cls and \"cls_state_dict\" in checkpoint:\n", "                cls_state_dict = checkpoint[\"cls_state_dict\"]\n", "                self.cls_head.load_state_dict(cls_state_dict)\n", "            if load_optim:\n", "                optim_state_dict = checkpoint[\"optim_state_dict\"]\n", "                self.optim.load_state_dict(optim_state_dict)\n", "                lr_pretrained = self.optim.param_groups[0][\"lr\"]\n", "                lr_config = self.config.optim_params.learning_rate\n", "                # Change learning rate\n", "                if not lr_pretrained == lr_config:\n", "                    for param_group in self.optim.param_groups:\n", "                        param_group[\"lr\"] = self.config.optim_params.learning_rate\n", "            self._init_memory_bank()\n", "            if (\n", "                load_memory_bank or self.config.model_params.load_memory_bank == False\n", "            ):  # load memory_bank\n", "                self._load_memory_bank(\n", "                    {\n", "                        \"source\": checkpoint[\"memory_bank_source\"],\n", "                        \"target\": checkpoint[\"memory_bank_target\"],\n", "                    }\n", "                )\n", "            self.logger.info(\n", "                f\"Checkpoint loaded successfully from '{filename}' at (epoch {checkpoint['epoch']}) at (iteration s:{checkpoint['iteration_source']} t:{checkpoint['iteration_target']}) with loss = {checkpoint['loss']}\\nval acc = {checkpoint['val_acc']}\\n\"\n", "            )\n", "        except OSError as e:\n", "            self.logger.info(f\"Checkpoint doesnt exists: [{filename}]\")\n", "            raise e\n", "    def save_checkpoint(self, filename=\"checkpoint.pth.tar\"):\n", "        out_dict = {\n", "            \"config\": self.config,\n", "            \"model_state_dict\": self.model.state_dict(),\n", "            \"optim_state_dict\": self.optim.state_dict(),\n", "            \"memory_bank_source\": self.get_attr(\"source\", \"memory_bank_wrapper\"),\n", "            \"memory_bank_target\": self.get_attr(\"target\", \"memory_bank_wrapper\"),\n", "            \"epoch\": self.current_epoch,\n", "            \"iteration\": self.current_iteration,\n", "            \"iteration_source\": self.get_attr(\"source\", \"current_iteration\"),\n", "            \"iteration_target\": self.get_attr(\"target\", \"current_iteration\"),\n", "            \"val_iteration\": self.current_val_iteration,\n", "            \"val_acc\": np.array(self.val_acc),\n", "            \"val_metric\": self.current_val_metric,\n", "            \"loss\": self.current_loss,\n", "            \"train_loss\": np.array(self.train_loss),\n", "        }\n", "        if self.cls:\n", "            out_dict[\"cls_state_dict\"] = self.cls_head.state_dict()\n", "        # best according to source-to-target\n", "        is_best = (\n", "            self.current_val_metric == self.best_val_metric\n", "        ) or not self.config.validate_freq\n", "        torchutils.save_checkpoint(\n", "            out_dict, is_best, filename=filename, folder=self.config.checkpoint_dir\n", "        )\n", "        self.copy_checkpoint()\n\n", "    # compute train features\n", "    @torch.no_grad()\n", "    def compute_train_features(self):\n", "        if self.is_features_computed:\n", "            return\n", "        else:\n", "            self.is_features_computed = True\n", "        self.model.eval()\n", "        for domain in (\"source\", \"target\"):\n", "            train_loader = self.get_attr(domain, \"train_init_loader\")\n", "            features, y, idx = [], [], []\n", "            tqdm_batch = tqdm(\n", "                total=len(train_loader), desc=f\"[Compute train features of {domain}]\"\n", "            )\n", "            for batch_i, (indices, images, labels) in enumerate(train_loader):\n", "                images = images.to(self.device)\n", "                feat = self.model(images)\n", "                feat = F.normalize(feat, dim=1)\n", "                features.append(feat)\n", "                y.append(labels)\n", "                idx.append(indices)\n", "                tqdm_batch.update()\n", "            tqdm_batch.close()\n", "            features = torch.cat(features)\n", "            y = torch.cat(y)\n", "            idx = torch.cat(idx).to(self.device)\n", "            self.set_attr(domain, \"train_features\", features)\n", "            self.set_attr(domain, \"train_labels\", y)\n", "            self.set_attr(domain, \"train_indices\", idx)\n", "    def clear_train_features(self):\n", "        self.is_features_computed = False\n\n", "    # Memory bank\n", "    @torch.no_grad()\n", "    def _init_memory_bank(self):\n", "        out_dim = self.config.model_params.out_dim\n", "        for domain_name in (\"source\", \"target\"):\n", "            data_len = self.get_attr(domain_name, \"train_len\")\n", "            memory_bank = MemoryBank(data_len, out_dim)\n", "            if self.config.model_params.load_memory_bank:\n", "                self.compute_train_features()\n", "                idx = self.get_attr(domain_name, \"train_indices\")\n", "                feat = self.get_attr(domain_name, \"train_features\")\n", "                memory_bank.update(idx, feat)\n", "                # self.logger.info(\n", "                #     f\"Initialize memorybank-{domain_name} with pretrained output features\"\n", "                # )\n", "                # save space\n", "                if self.config.data_params.name in [\"visda17\", \"domainnet\"]:\n", "                    delattr(self, f\"train_indices_{domain_name}\")\n", "                    delattr(self, f\"train_features_{domain_name}\")\n", "            self.set_attr(domain_name, \"memory_bank_wrapper\", memory_bank)\n", "            self.loss_fn.module.set_attr(domain_name, \"data_len\", data_len)\n", "            self.loss_fn.module.set_broadcast(\n", "                domain_name, \"memory_bank\", memory_bank.as_tensor()\n", "            )\n", "    @torch.no_grad()\n", "    def _update_memory_bank(self, domain_name, indices, new_data_memory):\n", "        memory_bank_wrapper = self.get_attr(domain_name, \"memory_bank_wrapper\")\n", "        memory_bank_wrapper.update(indices, new_data_memory)\n", "        updated_bank = memory_bank_wrapper.as_tensor()\n", "        self.loss_fn.module.set_broadcast(domain_name, \"memory_bank\", updated_bank)\n", "    def _load_memory_bank(self, memory_bank_dict):\n", "        \"\"\"load memory bank from checkpoint\n", "        Args:\n", "            memory_bank_dict (dict): memory_bank dict of source and target domain\n", "        \"\"\"\n", "        for domain_name in (\"source\", \"target\"):\n", "            memory_bank = memory_bank_dict[domain_name]._bank.cuda()\n", "            self.get_attr(domain_name, \"memory_bank_wrapper\")._bank = memory_bank\n", "            self.loss_fn.module.set_broadcast(domain_name, \"memory_bank\", memory_bank)\n\n", "    # Cluster\n", "    @torch.no_grad()\n", "    def _update_cluster_labels(self):\n", "        k_list = self.config.k_list\n", "        for clus_type in self.config.loss_params.clus.type:\n", "            cluster_labels_domain = {}\n", "            cluster_centroids_domain = {}\n", "            cluster_phi_domain = {}\n\n", "            # clustering for each domain\n", "            if clus_type == \"each\":\n", "                for domain_name in (\"source\", \"target\"):\n", "                    memory_bank_tensor = self.get_attr(\n", "                        domain_name, \"memory_bank_wrapper\"\n", "                    ).as_tensor()\n", "                    # clustering\n", "                    cluster_labels, cluster_centroids, cluster_phi = torch_kmeans(\n", "                        k_list,\n", "                        memory_bank_tensor,\n", "                        seed=self.current_epoch + self.current_iteration,\n", "                    )\n", "                    cluster_labels_domain[domain_name] = cluster_labels\n", "                    cluster_centroids_domain[domain_name] = cluster_centroids\n", "                    cluster_phi_domain[domain_name] = cluster_phi\n", "                self.cluster_each_centroids_domain = cluster_centroids_domain\n", "                self.cluster_each_labels_domain = cluster_labels_domain\n", "                self.cluster_each_phi_domain = cluster_phi_domain\n", "            else:\n", "                print(clus_type)\n", "                raise NotImplementedError\n\n", "            # update cluster to losss_fn\n", "            for domain_name in (\"source\", \"target\"):\n", "                self.loss_fn.module.set_broadcast(\n", "                    domain_name,\n", "                    f\"cluster_labels_{clus_type}\",\n", "                    cluster_labels_domain[domain_name],\n", "                )\n", "                self.loss_fn.module.set_broadcast(\n", "                    domain_name,\n", "                    f\"cluster_centroids_{clus_type}\",\n", "                    cluster_centroids_domain[domain_name],\n", "                )\n", "                if cluster_phi_domain:\n", "                    self.loss_fn.module.set_broadcast(\n", "                        domain_name,\n", "                        f\"cluster_phi_{clus_type}\",\n", "                        cluster_phi_domain[domain_name],\n", "                    )"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}