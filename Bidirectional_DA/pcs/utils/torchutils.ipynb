{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import logging\n", "import os\n", "import random\n", "import shutil\n", "import sys\n", "from subprocess import call"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torchvision\n", "from torch.autograd import Function"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def set_seed(seed=1234, determine=True):\n", "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n", "    random.seed(seed)\n", "    np.random.seed(seed)\n", "    torch.manual_seed(seed)\n", "    torch.cuda.manual_seed(seed)\n", "    torch.cuda.manual_seed_all(seed)\n", "    if determine:\n", "        torch.backends.cudnn.deterministic = True\n", "        torch.backends.cudnn.benchmark = False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Tensor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def expand_1d(t, num_reps):\n", "    return t.unsqueeze(1).expand(-1, num_reps)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def isin(ar1, ar2):\n", "    # for every element of ar2, is ar2 in ar1\n", "    # return shape same to ar1\n", "    return (ar1[..., None] == ar2).any(-1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def dot(x, y):\n", "    return torch.sum(x * y, dim=-1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def contrastive_sim(instances, proto=None, tao=0.05):\n", "    # prob_matrix [bs, dim]\n", "    # proto_dim [nums, dim]\n", "    if proto is None:\n", "        proto = instances\n", "    ins_ext = instances.unsqueeze(1).repeat(1, proto.size(0), 1)\n", "    sim_matrix = torch.exp(dot(ins_ext, proto) / tao)\n", "    return sim_matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def contrastive_sim_z(instances, proto=None, tao=0.05):\n", "    sim_matrix = contrastive_sim(instances, proto, tao)\n", "    return torch.sum(sim_matrix, dim=-1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def contrastive_prob(instances, proto=None, tao=0.05):\n", "    sim_matrix = contrastive_sim(instances, proto, tao)\n", "    return sim_matrix / torch.sum(sim_matrix, dim=-1).unsqueeze(-1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def pairwise_distance_2(input_1, input_2):\n", "    assert input_1.size(1) == input_2.size(1)\n", "    dis_vec = input_1.unsqueeze(1) - input_2\n", "    dis = torch.norm(dis_vec, dim=2)\n", "    return dis"]}, {"cell_type": "markdown", "metadata": {}, "source": ["nn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def weights_init(model):\n", "    for layer in model.modules():\n", "        if isinstance(layer, torch.nn.Conv2d):\n", "            torch.nn.init.kaiming_normal_(\n", "                layer.weight, mode=\"fan_out\", nonlinearity=\"relu\"\n", "            )\n", "            if layer.bias is not None:\n", "                torch.nn.init.constant_(layer.bias, val=0.0)\n", "        elif isinstance(layer, torch.nn.BatchNorm2d):\n", "            torch.nn.init.constant_(layer.weight, val=1.0)\n", "            torch.nn.init.constant_(layer.bias, val=0.0)\n", "        elif isinstance(layer, torch.nn.Linear):\n", "            torch.nn.init.xavier_normal_(layer.weight)\n", "            if layer.bias is not None:\n", "                torch.nn.init.constant_(layer.bias, val=0.0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def split_params_by_name(model, name):\n", "    if not isinstance(name, list):\n", "        name = [name]\n", "    with_name = []\n", "    without_name = []\n", "    for key, param in model.named_parameters():\n", "        if not param.requires_grad:\n", "            continue\n", "        in_key = False\n", "        for n in name:\n", "            in_key = in_key | (n in key)\n", "        if in_key:\n", "            with_name.append(param)\n", "        else:\n", "            without_name.append(param)\n", "    return with_name, without_name"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class GradReverse(Function):\n", "    @staticmethod\n", "    def forward(ctx, x, lambd=1.0):\n", "        ctx.lambd = lambd\n", "        return x.view_as(x)\n", "    @staticmethod\n", "    def backward(ctx, grad_output):\n", "        return grad_output * -ctx.lambd, None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def grad_reverse(x, lambd=1.0):\n", "    return GradReverse.apply(x, lambd)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["nn.functional"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def entropy(x, eps=1e-5):\n", "    p = F.softmax(x, dim=-1)\n", "    entropy = -torch.mean(torch.sum(p * torch.log(p + eps), 1))\n", "    return entropy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def pseudo_mask(x, thres=0.95):\n", "    prob = F.softmax(x, dim=1)\n", "    max_prob, pred = prob.max(dim=1)\n", "    mask = max_prob > thres\n", "    return mask"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def pseudo_label_loss(x, thres=0.95, aux=True, y=None, mask=None, num_class=10):\n", "    if mask is None:\n", "        mask = [True] * len(x)\n", "        mask = torch.tensor(mask)\n", "    # update mask\n", "    prob = F.softmax(x, dim=1)\n", "    max_prob, pred = prob.max(dim=1)\n", "    mask[max_prob < thres] = False\n", "    num_thres = mask.sum().item()\n\n", "    # calculate loss\n", "    out_thres, pred_thres = x[mask], pred[mask]\n", "    if num_thres == 0:\n", "        loss = torch.tensor(0)\n", "    else:\n", "        loss = F.cross_entropy(out_thres, pred_thres)\n", "    if aux:\n", "        num_select_per_class = [0] * num_class\n", "        num_correct_per_class = [0] * num_class\n", "        for i in range(num_class):\n", "            num_select_per_class[i] += (pred_thres == i).sum().item()\n", "            if y is not None:\n", "                num_correct_per_class[i] += (\n", "                    ((pred_thres == i) & (pred_thres.eq(y[mask]))).sum().item()\n", "                )\n", "        if y is not None:\n", "            num_correct = pred_thres.eq(y[mask]).sum().item()\n", "        else:\n", "            num_correct = -1\n", "        ret_aux = {\n", "            \"num_select\": num_thres,\n", "            \"num_correct\": num_correct,\n", "            \"num_select_per_class\": num_select_per_class,\n", "            \"num_correct_per_class\": num_correct_per_class,\n", "            \"mask\": mask,\n", "        }\n", "        return loss, ret_aux\n", "    else:\n", "        return loss"]}, {"cell_type": "markdown", "metadata": {}, "source": ["optim"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def lr_scheduler_invLR(optimizer, gamma=0.0001, power=0.75):\n", "    def lmbda(iter):\n", "        return (1 + gamma * iter) ** (-power)\n", "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lmbda)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_lr(optimizer, g_id=0):\n", "    return optimizer.param_groups[g_id][\"lr\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["utils"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def copy_checkpoint(\n", "    folder=\"./\", filename=\"checkpoint.pth.tar\", copyname=\"copy.pth.tar\"\n", "):\n", "    shutil.copyfile(os.path.join(folder, filename), os.path.join(folder, copyname))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def save_checkpoint(state, is_best=False, folder=\"./\", filename=\"checkpoint.pth.tar\"):\n", "    if not os.path.isdir(folder):\n", "        os.mkdir(folder)\n", "    torch.save(state, os.path.join(folder, filename))\n", "    if is_best:\n", "        copy_checkpoint(folder, filename, \"model_best.pth.tar\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_state_dict(model, model_dict):\n", "    model_dict = model.state_dict()\n", "    updated_dict = {k: v for k, v in model_dict.items() if k in model_dict.keys()}\n", "    model_dict.update(updated_dict)\n", "    model.load_state_dict(model_dict)\n", "    return len(updated_dict.keys())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def print_cuda_statistics(nvidia_smi=True, output=print):\n", "    output(f\"Python VERSION: {sys.version}\")\n", "    output(f\"pytorch VERSION: {torch.__version__}\")\n", "    output(f\"CUDA VERSION: {torch.version.cuda}\")\n", "    output(f\"CUDNN VERSION: {torch.backends.cudnn.version()}\")\n", "    output(f\"Device NAME: {torch.cuda.get_device_name(0)}\")\n", "    output(f\"Number CUDA Devices: {torch.cuda.device_count()}\")\n", "    output(f\"Available devices: {torch.cuda.device_count()}\")\n", "    output(f\"current CUDA Device: {torch.cuda.current_device()}\")\n", "    if nvidia_smi:\n", "        print(\"nvidia-smi:\")\n", "        call(\n", "            [\n", "                \"nvidia-smi\",\n", "                \"--format=csv\",\n", "                \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\",\n", "            ]\n", "        )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def log_tensor(t, name=\"\", print_tensor=False):\n", "    print(\n", "        f\"Tensor {name}:\\n\\ttype: {t.type()}\\n\\tsize {t.shape}\\n\\tdim: {t.dim()}\\n\\tdevice: {t.device}\\n\\tnelement: {t.nelement()}\\n\\telem_size: {t.element_size()}\\n\\tsize in mem: {t.nelement() * t.element_size()} Bytes\\n\\tgrad_fn: {t.grad_fn}\\n\\tgrad: {t.grad}\"\n", "    )\n", "    if print_tensor:\n", "        print(t)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def model_params_num(model):\n", "    return sum(torch.numel(parameter) for parameter in model.parameters())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def one_hot(label):\n", "    N = label.size(0)\n", "    num_classes = label.unique().size(0)\n", "    one_hot = torch.zeros(N, num_classes).long()\n", "    one_hot.scatter_(\n", "        dim=1,\n", "        index=torch.unsqueeze(label, dim=1),\n", "        src=torch.ones(N, num_classes).long(),\n", "    )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def top_k_for_each_class(pred, prob, num_class):\n", "    ind = torch.arange(len(pred)).long()\n", "    pred_ret = torch.zeros_like(pred).long().cuda() - 1\n", "    for i in range(num_class):\n", "        class_mask = pred == i\n", "        num_c = class_mask.sum()\n", "        num_c = class_mask.sum()\n", "        if num_c == 0:\n", "            continue\n", "        prob_class = prob[class_mask]\n", "        ind_class = ind[class_mask]\n", "        prob_topk, ind_topk = prob_class.topk(min(5, num_c))\n", "        ind_topk = ind_class[ind_topk]\n", "        pred_ret[ind_topk] = i\n", "    return pred_ret"]}, {"cell_type": "markdown", "metadata": {}, "source": ["MIM"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MomentumSoftmax:\n", "    def __init__(self, num_class, m=1):\n", "        self.softmax_vector = torch.zeros(num_class).detach() + 1.0 / num_class\n", "        self.m = m\n", "        self.num = m\n", "    def update(self, mean_softmax, num=1):\n", "        self.softmax_vector = (\n", "            (self.softmax_vector * self.num) + mean_softmax * num\n", "        ) / (self.num + num)\n", "        self.num += num\n", "    def reset(self):\n", "        # print(self.softmax_vector)\n", "        self.num = self.m"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}