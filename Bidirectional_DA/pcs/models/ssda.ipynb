{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import random"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import torch\n", "import torch.cuda.comm\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torchvision\n", "from pcs.utils import reverse_domain, torchutils"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SSDALossModule(torch.nn.Module):\n", "    def __init__(self, config, gpu_devices):\n", "        super(SSDALossModule, self).__init__()\n", "        self.config = config\n", "        self.gpu_devices = gpu_devices\n", "        self.k = config.loss_params.k\n", "        self.t = config.loss_params.temp\n", "        self.m = config.loss_params.m\n", "        self.loss_w = self.config.loss_params.weight\n", "        self.loss = self.config.loss_params.loss\n", "        self.indices = None\n", "        self.outputs = None\n", "        self.indices_random = None\n", "        self.batch_size = None\n", "        self.broadcast_name = []\n", "    def get_attr(self, domain, name):\n", "        return getattr(self, name + \"_\" + domain)\n", "    def set_attr(self, domain, name, value):\n", "        setattr(self, name + \"_\" + domain, value)\n", "        return self.get_attr(domain, name)\n", "    def set_broadcast(self, domain_name, name, value, comm=False):\n", "        if isinstance(value, list):\n", "            value = value.copy()\n", "            for i in range(len(value)):\n", "                value[i] = torch.cuda.comm.broadcast(value[i], self.gpu_devices)\n", "        else:\n", "            value = torch.cuda.comm.broadcast(value, self.gpu_devices)\n", "        self.set_attr(domain_name, f\"{name}_broadcast\", value)\n", "        self.set_attr(domain_name, name, None)\n", "        self.broadcast_name.append((domain_name, name))\n", "    def get_broadcast(self, domain_name, name, gpu_idx):\n", "        broadcast = self.get_attr(domain_name, f\"{name}_broadcast\")\n", "        if len(broadcast) == 0 or isinstance(broadcast[0], list):\n", "            tmp = []\n", "            for x in broadcast:\n", "                tmp.append(x[gpu_idx])\n", "            self.set_attr(domain_name, name, tmp)\n", "        else:\n", "            tmp = broadcast[gpu_idx]\n", "            self.set_attr(domain_name, name, tmp)\n", "        return tmp\n", "    def _identify_gpu(self, gpu_idx):\n", "        for domain_name, name in self.broadcast_name:\n", "            self.get_broadcast(domain_name, name, gpu_idx)\n\n", "    # memory bank calculation function\n", "    @torch.no_grad()\n", "    def updated_new_data_memory(self, domain, indices, outputs):\n", "        \"\"\"Compute new memory bank in indices by momentum\n", "        Args:\n", "            indices: indices of memory bank features to update\n", "            outputs: output of features\n", "            domain (str): 'source', 'target'\n", "        \"\"\"\n", "        memory_bank = self.get_attr(domain, \"memory_bank\")\n", "        data_memory = torch.index_select(memory_bank, 0, indices)\n", "        outputs = F.normalize(outputs, dim=1)\n", "        m = self.m\n", "        new_data_memory = data_memory * m + (1 - m) * outputs\n", "        return F.normalize(new_data_memory, dim=1)\n", "    def _get_Z(self, domain, vec, t):\n", "        \"\"\"Get denominator in ID\n", "        Args:\n", "            vec: output features [batch_size, dim]\n", "            domain (str): 'source', 'target'\n", "        Returns:\n", "            [batch_size] denominator in ID\n", "        \"\"\"\n", "        bank = self.get_attr(domain, \"memory_bank\")  # [data_len]\n", "        Z = torchutils.contrastive_sim_z(vec, bank, tao=t)\n", "        return Z\n", "    def _get_all_dot_products(self, domain, vec):\n", "        \"\"\"get dot product with all vectors in memory bank\n", "        Args:\n", "            vec: [bs, dim]\n", "            domain (str): 'source', 'target'\n", "        Returns:\n", "            [bs, data_len]\n", "        \"\"\"\n", "        assert len(vec.size()) == 2\n", "        bank = self.get_attr(domain, \"memory_bank\")\n", "        return torch.matmul(vec, torch.transpose(bank, 1, 0))\n", "    def _compute_I2C_loss(self, domain, loss_type, t=0.05):\n", "        \"\"\"Loss CrossSelf in essay (Cross-domain Instance-Prototype SSL)\n", "        Args:\n", "            domain (str): 'source', 'target'\n", "            loss_type (str, optional): 'each', 'all', 'src', 'tgt'. Defaults to \"zero\".\n", "        \"\"\"\n", "        assert loss_type in [\"cross\", \"tgt\", \"src\"]\n", "        loss = torch.Tensor([0]).cuda()\n", "        if (loss_type == \"tgt\" and domain == \"source\") or (\n", "            loss_type == \"src\" and domain == \"target\"\n", "        ):\n", "            return loss\n", "        clus = \"each\"\n", "        k_list = self.config.k_list\n", "        n_kmeans = len(k_list)\n", "        cluster_centroids = self.get_attr(\n", "            reverse_domain(domain), f\"cluster_centroids_{clus}\"\n", "        )\n", "        outputs = self.outputs\n", "        for each_k_idx, k in enumerate(k_list):\n", "            centroids = cluster_centroids[each_k_idx]\n", "            phi = t\n", "            p = torchutils.contrastive_sim(outputs, centroids, tao=phi)\n", "            z = torch.sum(p, dim=-1)  # [bs]\n", "            p = p / z.unsqueeze(1)  # [bs, k]\n", "            cur_loss = -torch.sum(p * torch.log(p)) / self.batch_size\n", "            loss = loss + cur_loss\n", "        loss /= n_kmeans\n", "        return loss\n", "    def _compute_proto_loss(self, domain, loss_type, t=0.05):\n", "        \"\"\"Loss PC in essay (part of In-domain Prototypical Contrastive Learning)\n", "        Args:\n", "            domain (str): 'source', 'target'\n", "            loss_type (str, optional): 'each', 'all', 'src', 'tgt'. Defaults to \"zero\".\n", "        \"\"\"\n", "        loss = torch.Tensor([0]).cuda()\n", "        if (loss_type.startswith(\"src\") and domain == \"target\") or (\n", "            loss_type.startswith(\"tgt\") and domain == \"source\"\n", "        ):\n", "            return loss\n", "        is_fix = \"fix\" in loss_type\n", "        clus = \"each\"\n", "        n_kmeans = self.config.loss_params.clus.n_kmeans\n", "        k_list = self.config.k_list\n", "        c_domain = domain\n", "        cluster_labels = self.get_attr(domain, f\"cluster_labels_{clus}\")\n", "        cluster_centroids = self.get_attr(c_domain, f\"cluster_centroids_{clus}\")\n", "        cluster_phi = self.get_attr(c_domain, f\"cluster_phi_{clus}\")\n", "        for each_k_idx, k in enumerate(k_list):\n", "            # clus info\n", "            labels = cluster_labels[each_k_idx]\n", "            centroids = cluster_centroids[each_k_idx]\n", "            phis = cluster_phi[each_k_idx]\n\n", "            # batch info\n", "            batch_labels = labels[self.indices]\n", "            outputs = self.outputs\n", "            batch_centroids = centroids[batch_labels]\n", "            if loss_type == \"fix\":\n", "                batch_phis = t\n", "            else:\n", "                batch_phis = phis[batch_labels]\n\n", "            # calculate similarity\n", "            dot_exp = torch.exp(\n", "                torch.sum(outputs * batch_centroids, dim=-1) / batch_phis\n", "            )\n", "            assert not torch.isnan(outputs).any()\n", "            assert not torch.isnan(batch_centroids).any()\n", "            assert not torch.isnan(dot_exp).any()\n\n", "            # calculate Z\n", "            all_phi = t if is_fix else phis.unsqueeze(0).repeat(outputs.shape[0], 1)\n", "            z = torchutils.contrastive_sim_z(outputs, centroids, tao=all_phi)\n\n", "            # calculate loss\n", "            p = dot_exp / z\n", "            loss = loss - torch.sum(torch.log(p)) / p.size(0)\n", "        loss /= n_kmeans\n", "        return loss\n", "    def _compute_CD_loss(self, domain, loss_type, t=0.05):\n", "        \"\"\"Loss CDS in essay arXiv:2003.08264v1, not used in essay\n", "        Args:\n", "            domain (str): different domain from current one\n", "            loss_type (str): 'cross'\n", "        Returns:\n", "            CD loss\n", "        \"\"\"\n", "        assert loss_type in [\"cross\"]\n", "        bank = self.get_attr(reverse_domain(domain), \"memory_bank\")\n", "        if self.config.loss_params.sample_ratio:\n", "            num_sample = int(self.config.loss_params.sample_ratio * len(bank))\n", "            sampled_index = random.sample(list(range(len(bank))), num_sample)\n", "            sampled_index = torch.tensor(sampled_index)\n", "            bank = bank[sampled_index]\n\n", "        # [bs, data_len], numerator of P_{i',i}^{s->t}\n", "        prods = torchutils.contrastive_sim(self.outputs, bank, tao=t)\n", "        # [bs]\n", "        z = torch.sum(prods, dim=-1)\n", "        # [bs, data_len] P_{i',i}^{s->t}\n", "        p = prods / z.unsqueeze(1)\n", "        aux = p.max(dim=1)[1]\n", "        # double sum\n", "        loss = -torch.sum(p * torch.log(p)) / self.batch_size\n", "        return loss[None,], aux\n", "    def _compute_ID_loss(self, domain, loss_type, t=0.05):\n", "        \"\"\"Loss ID (Instance Discrimination), not used in essay.\n", "        Args:\n", "            domain (str): 'source', 'target'\n", "            loss_type (str): 'each', 'all'\n", "        Returns:\n", "            ID loss\n", "        \"\"\"\n", "        assert loss_type in [\"all\", \"each\", \"src\", \"tgt\"]\n", "        loss = torch.Tensor([0]).cuda()\n", "        if loss_type == \"src\":\n", "            if domain == \"target\":\n", "                return loss\n", "            else:\n", "                clus = \"each\"\n", "        if loss_type == \"tgt\":\n", "            if domain == \"source\":\n", "                return loss\n", "            else:\n", "                clus = \"each\"\n", "        bank = self.get_attr(domain, \"memory_bank\")\n", "        memory_vecs = torch.index_select(bank, 0, self.indices)\n", "        prods = torch.sum(memory_vecs * self.outputs, dim=-1)\n", "        # [bs], numerator of P_i^s\n", "        prods_exp = torch.exp(prods / t)\n", "        # [bs], denominator of P_i^s\n", "        if self.config.loss_params.sample_ratio:\n", "            num_sample = int(self.config.loss_params.sample_ratio * len(bank))\n", "            sampled_index = random.sample(list(range(len(bank))), num_sample)\n", "            sampled_index = torch.tensor(sampled_index)\n", "            sampled_bank = bank[sampled_index]\n", "            ratio_inv = 1 / self.config.loss_params.sample_ratio\n", "        else:\n", "            sampled_bank = bank\n", "            ratio_inv = 1\n", "        Z = torchutils.contrastive_sim_z(self.outputs, sampled_bank, tao=t)\n", "        Z = ratio_inv * Z\n", "        if loss_type == \"all\":\n", "            bank_rev = self.get_attr(reverse_domain(domain), \"memory_bank\")\n", "            Z = Z + torchutils.contrastive_sim_z(self.outputs, bank_rev, tao=t)\n", "        # [bs], P_i^s\n", "        p = prods_exp / Z\n", "        loss = -torch.sum(torch.log(p)) / self.batch_size\n", "        return loss[\n", "            None,\n", "        ]\n", "    def _compute_loss(self, domain, loss_type=None, t=0.05):\n", "        loss_name, loss_args = loss_type.split(\"-\")\n", "        loss_fn = getattr(self, f\"_compute_{loss_name}_loss\")\n", "        aux = None\n", "        if loss_name in [\"CD\"]:\n", "            loss, aux = loss_fn(domain, loss_type=loss_args, t=t)\n", "        else:\n", "            loss = loss_fn(domain, loss_type=loss_args, t=t)\n", "        assert not torch.isinf(loss).any()\n", "        if loss < 0:\n", "            print(loss)\n", "            print(loss_name)\n", "            assert loss >= 0\n", "        return loss, aux\n", "    def forward(self, indices, outputs, domain, gpu_idx):\n", "        self.indices = indices.detach()\n", "        self.batch_size = self.indices.size(0)\n", "        self.outputs = outputs\n", "        self._identify_gpu(gpu_idx)\n", "        loss_part = []\n", "        loss = torch.zeros(1).cuda()\n", "        aux_list = {}\n", "        for i, ls in enumerate(self.loss):\n", "            if (\n", "                self.epoch <= self.config.loss_params.start[i]\n", "                or self.epoch >= self.config.loss_params.end[i]\n", "                or ls.split(\"-\")[0] in [\"cls\", \"tgt\", \"semi\", \"norm\"]\n", "            ):\n", "                l = torch.zeros(1).cuda()\n", "            else:\n", "                l, aux = self._compute_loss(domain, ls, self.t[i])\n", "                aux_list[ls] = aux\n", "            loss_part.append(l)\n", "            loss = loss + l\n", "        new_data_memory = self.updated_new_data_memory(domain, indices, outputs)\n", "        return loss, new_data_memory, loss_part, aux_list"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def loss_info(feat, mb_feat, label, t=0.1):\n", "    loss = torch.Tensor([0]).cuda()\n", "    z = torchutils.contrastive_sim_z(feat, mb_feat, tao=t)\n", "    for i, lb in enumerate(label):\n", "        pos = mb_feat[label == lb]\n", "        up = torch.exp(torchutils.dot(feat[i], pos) / t)\n", "        p = up / z[i]\n", "        assert all(p < 1)\n", "        loss += -torch.sum(torch.log(p)) / len(p)\n", "    loss /= len(feat)\n", "    return loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@torch.no_grad()\n", "def update_data_memory(data_memory, outputs, m=0.9):\n", "    outputs = F.normalize(outputs, dim=1)\n", "    new_data_memory = data_memory * m + (1 - m) * outputs\n", "    return F.normalize(new_data_memory, dim=1)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}